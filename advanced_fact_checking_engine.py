#!/usr/bin/env python3
"""
üî¨ ADVANCED FACT-CHECKING ENGINE
================================

Comprehensive fact verification system with multiple validation layers,
source cross-referencing, and confidence scoring for Codex Eternum Omega.
"""

import asyncio
import aiohttp
import requests
import json
from typing import Dict, List, Tuple, Optional, Union
from datetime import datetime, timedelta
import numpy as np
import pandas as pd
from dataclasses import dataclass, asdict
import hashlib
import logging
import re
from urllib.parse import urlparse, urljoin
import sqlite3
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import yfinance as yf
from textblob import TextBlob
import feedparser

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class VerificationSource:
    """Configuration for verification data sources"""
    name: str
    base_url: str
    api_key: Optional[str]
    reliability_score: float  # 0.0 to 1.0
    rate_limit: int  # requests per minute
    data_types: List[str]
    headers: Dict[str, str]
    last_used: datetime = None

@dataclass 
class FactCheckQuery:
    """Structured fact-checking query"""
    claim: str
    claim_type: str  # financial, economic, corporate, statistical, news
    entities: List[str]  # extracted entities (companies, indicators, etc.)
    numeric_values: List[float]  # extracted numbers
    date_references: List[str]  # extracted dates
    confidence_required: float  # minimum confidence threshold
    sources_required: int  # minimum number of sources

@dataclass
class VerificationEvidence:
    """Evidence collected during verification"""
    source: str
    data: Dict
    confidence: float
    timestamp: datetime
    verification_method: str
    supporting: bool  # True if supports claim, False if contradicts

@dataclass
class ComprehensiveFactCheck:
    """Complete fact-check result with all evidence"""
    query: FactCheckQuery
    overall_verified: bool
    confidence_score: float
    risk_assessment: str  # LOW, MEDIUM, HIGH
    evidence_quality: str  # STRONG, MODERATE, WEAK
    supporting_evidence: List[VerificationEvidence]
    contradicting_evidence: List[VerificationEvidence]
    verification_timestamp: datetime
    processing_time: float
    sources_consulted: int
    warnings: List[str]
    recommendations: List[str]

class AdvancedFactChecker:
    """Comprehensive fact-checking system with multiple verification layers"""
    
    def __init__(self):
        self.verification_sources = self._initialize_sources()
        self.verification_cache = {}
        self.entity_extractor = EntityExtractor()
        self.numeric_validator = NumericValidator()
        self.cross_referencer = CrossReferencer()
        
        # Database for caching and historical tracking
        self._setup_database()
        
    def _initialize_sources(self) -> List[VerificationSource]:
        """Initialize comprehensive verification sources"""
        return [
            VerificationSource(
                name="Reuters_API",
                base_url="https://api.reuters.com/v1/",
                api_key=None,  # Would be configured in production
                reliability_score=0.95,
                rate_limit=1000,
                data_types=["news", "finance", "markets", "corporate"],
                headers={"User-Agent": "CodexOmega-FactChecker/1.0"}
            ),
            VerificationSource(
                name="Bloomberg_Terminal",
                base_url="https://api.bloomberg.com/",
                api_key=None,
                reliability_score=0.96,
                rate_limit=500,
                data_types=["finance", "economics", "markets", "corporate"],
                headers={"Accept": "application/json"}
            ),
            VerificationSource(
                name="Yahoo_Finance",
                base_url="https://query1.finance.yahoo.com/",
                api_key=None,
                reliability_score=0.88,
                rate_limit=2000,
                data_types=["stocks", "finance", "markets"],
                headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
            ),
            VerificationSource(
                name="Federal_Reserve_FRED",
                base_url="https://api.stlouisfed.org/fred/",
                api_key=None,  # Would require FRED API key
                reliability_score=0.98,
                rate_limit=1000,
                data_types=["economic_indicators", "monetary_policy", "statistics"],
                headers={"Accept": "application/json"}
            ),
            VerificationSource(
                name="SEC_EDGAR",
                base_url="https://data.sec.gov/",
                api_key=None,
                reliability_score=0.99,
                rate_limit=100,
                data_types=["corporate_filings", "financial_statements", "insider_trading"],
                headers={"User-Agent": "CodexOmega factchecker@aistorelab.com"}
            ),
            VerificationSource(
                name="World_Bank_API",
                base_url="https://api.worldbank.org/v2/",
                api_key=None,
                reliability_score=0.96,
                rate_limit=1000,
                data_types=["economic_data", "development_indicators", "country_stats"],
                headers={"Accept": "application/json"}
            ),
            VerificationSource(
                name="BLS_Labor_Statistics",
                base_url="https://api.bls.gov/publicAPI/v2/",
                api_key=None,
                reliability_score=0.97,
                rate_limit=500,
                data_types=["employment", "inflation", "wages", "productivity"],
                headers={"Content-Type": "application/json"}
            ),
            VerificationSource(
                name="Census_Bureau",
                base_url="https://api.census.gov/",
                api_key=None,
                reliability_score=0.95,
                rate_limit=500,
                data_types=["demographics", "economic_indicators", "population"],
                headers={"Accept": "application/json"}
            ),
            VerificationSource(
                name="Financial_Times_API",
                base_url="https://api.ft.com/",
                api_key=None,
                reliability_score=0.92,
                rate_limit=300,
                data_types=["news", "finance", "markets", "analysis"],
                headers={"Accept": "application/json"}
            ),
            VerificationSource(
                name="Quandl_Financial",
                base_url="https://www.quandl.com/api/v3/",
                api_key=None,
                reliability_score=0.90,
                rate_limit=2000,
                data_types=["financial_data", "economic_data", "alternative_data"],
                headers={"Accept": "application/json"}
            )
        ]
    
    def _setup_database(self):
        """Setup SQLite database for caching and tracking"""
        try:
            self.conn = sqlite3.connect('fact_verification.db', check_same_thread=False)
            self.conn.execute('''
                CREATE TABLE IF NOT EXISTS verification_cache (
                    claim_hash TEXT PRIMARY KEY,
                    claim_text TEXT,
                    verification_result TEXT,
                    confidence_score REAL,
                    timestamp TEXT,
                    sources_used TEXT
                )
            ''')
            
            self.conn.execute('''
                CREATE TABLE IF NOT EXISTS source_reliability (
                    source_name TEXT PRIMARY KEY,
                    success_rate REAL,
                    average_response_time REAL,
                    last_updated TEXT,
                    total_queries INTEGER
                )
            ''')
            
            self.conn.commit()
            logger.info("Fact-checking database initialized successfully")
            
        except Exception as e:
            logger.error(f"Database setup error: {str(e)}")
            self.conn = None
    
    async def comprehensive_fact_check(self, claim: str, claim_type: str = "auto", 
                                     confidence_threshold: float = 0.85,
                                     sources_required: int = 3) -> ComprehensiveFactCheck:
        """
        Perform comprehensive fact-checking with multiple verification layers
        """
        start_time = time.time()
        
        try:
            # Step 1: Parse and structure the claim
            query = self._parse_claim(claim, claim_type, confidence_threshold, sources_required)
            
            # Step 2: Check cache first
            cached_result = self._check_cache(claim)
            if cached_result and self._is_cache_valid(cached_result):
                logger.info(f"Using cached verification for claim: {claim[:50]}...")
                return cached_result
            
            # Step 3: Gather evidence from multiple sources
            evidence_tasks = []
            for source in self.verification_sources:
                if self._can_verify_claim_type(source, query.claim_type):
                    task = self._verify_with_source(source, query)
                    evidence_tasks.append(task)
            
            # Execute verification tasks concurrently
            all_evidence = []
            async with aiohttp.ClientSession() as session:
                results = await asyncio.gather(*evidence_tasks, return_exceptions=True)
                
                for result in results:
                    if isinstance(result, Exception):
                        logger.warning(f"Verification task failed: {str(result)}")
                    elif result:
                        all_evidence.extend(result)
            
            # Step 4: Cross-reference and validate evidence
            supporting_evidence = [e for e in all_evidence if e.supporting]
            contradicting_evidence = [e for e in all_evidence if not e.supporting]
            
            # Step 5: Calculate confidence and make verification decision
            overall_confidence = self._calculate_overall_confidence(supporting_evidence, contradicting_evidence)
            overall_verified = overall_confidence >= confidence_threshold and len(supporting_evidence) >= sources_required
            
            # Step 6: Risk assessment
            risk_assessment = self._assess_risk(overall_confidence, len(contradicting_evidence), len(supporting_evidence))
            evidence_quality = self._assess_evidence_quality(all_evidence)
            
            # Step 7: Generate warnings and recommendations
            warnings = self._generate_warnings(query, all_evidence, overall_confidence)
            recommendations = self._generate_recommendations(query, all_evidence, overall_verified)
            
            # Step 8: Create comprehensive result
            result = ComprehensiveFactCheck(
                query=query,
                overall_verified=overall_verified,
                confidence_score=overall_confidence,
                risk_assessment=risk_assessment,
                evidence_quality=evidence_quality,
                supporting_evidence=supporting_evidence,
                contradicting_evidence=contradicting_evidence,
                verification_timestamp=datetime.now(),
                processing_time=time.time() - start_time,
                sources_consulted=len(self.verification_sources),
                warnings=warnings,
                recommendations=recommendations
            )
            
            # Step 9: Cache the result
            self._cache_result(claim, result)
            
            logger.info(f"Fact-check completed: {claim[:50]}... | Verified: {overall_verified} | Confidence: {overall_confidence:.2%}")
            return result
            
        except Exception as e:
            logger.error(f"Comprehensive fact-check failed: {str(e)}")
            # Return error result
            return ComprehensiveFactCheck(
                query=FactCheckQuery(claim, "error", [], [], [], confidence_threshold, sources_required),
                overall_verified=False,
                confidence_score=0.0,
                risk_assessment="HIGH",
                evidence_quality="UNKNOWN",
                supporting_evidence=[],
                contradicting_evidence=[],
                verification_timestamp=datetime.now(),
                processing_time=time.time() - start_time,
                sources_consulted=0,
                warnings=[f"Verification system error: {str(e)}"],
                recommendations=["Manual verification recommended due to system error"]
            )
    
    def _parse_claim(self, claim: str, claim_type: str, confidence_threshold: float, sources_required: int) -> FactCheckQuery:
        """Parse claim into structured query"""
        
        # Auto-detect claim type if not specified
        if claim_type == "auto":
            claim_type = self._detect_claim_type(claim)
        
        # Extract entities (companies, indicators, etc.)
        entities = self.entity_extractor.extract_entities(claim)
        
        # Extract numeric values
        numeric_values = self.numeric_validator.extract_numbers(claim)
        
        # Extract date references
        date_references = self._extract_dates(claim)
        
        return FactCheckQuery(
            claim=claim,
            claim_type=claim_type,
            entities=entities,
            numeric_values=numeric_values,
            date_references=date_references,
            confidence_required=confidence_threshold,
            sources_required=sources_required
        )
    
    def _detect_claim_type(self, claim: str) -> str:
        """Automatically detect the type of claim"""
        claim_lower = claim.lower()
        
        financial_keywords = ['stock', 'price', 'market', 'trading', 'shares', 'dividend', 'earnings']
        economic_keywords = ['gdp', 'inflation', 'unemployment', 'interest rate', 'fed', 'economy']
        corporate_keywords = ['company', 'ceo', 'merger', 'acquisition', 'revenue', 'profit', 'quarterly']
        statistical_keywords = ['percentage', 'percent', 'rate', 'average', 'median', 'statistics']
        
        if any(keyword in claim_lower for keyword in financial_keywords):
            return "financial"
        elif any(keyword in claim_lower for keyword in economic_keywords):
            return "economic"
        elif any(keyword in claim_lower for keyword in corporate_keywords):
            return "corporate"
        elif any(keyword in claim_lower for keyword in statistical_keywords):
            return "statistical"
        else:
            return "general"
    
    def _extract_dates(self, claim: str) -> List[str]:
        """Extract date references from claim"""
        date_patterns = [
            r'\b\d{4}-\d{2}-\d{2}\b',  # YYYY-MM-DD
            r'\b\d{2}/\d{2}/\d{4}\b',  # MM/DD/YYYY
            r'\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]* \d{1,2},? \d{4}\b',  # Month DD, YYYY
            r'\bQ[1-4] \d{4}\b',  # Q1 2024
            r'\b\d{4} Q[1-4]\b'   # 2024 Q1
        ]
        
        dates = []
        for pattern in date_patterns:
            matches = re.findall(pattern, claim, re.IGNORECASE)
            dates.extend(matches)
        
        return dates
    
    async def _verify_with_source(self, source: VerificationSource, query: FactCheckQuery) -> List[VerificationEvidence]:
        """Verify claim with specific source"""
        evidence = []
        
        try:
            # Rate limiting check
            if self._is_rate_limited(source):
                logger.warning(f"Rate limited for source: {source.name}")
                return evidence
            
            # Source-specific verification logic
            if source.name == "Yahoo_Finance" and query.claim_type == "financial":
                evidence.extend(await self._verify_yahoo_finance(source, query))
            elif source.name == "Federal_Reserve_FRED" and query.claim_type == "economic":
                evidence.extend(await self._verify_fred(source, query))
            elif source.name == "SEC_EDGAR" and query.claim_type == "corporate":
                evidence.extend(await self._verify_sec_edgar(source, query))
            # Add more source-specific handlers as needed
            
            # Update source usage tracking
            source.last_used = datetime.now()
            
        except Exception as e:
            logger.error(f"Verification failed for source {source.name}: {str(e)}")
        
        return evidence
    
    async def _verify_yahoo_finance(self, source: VerificationSource, query: FactCheckQuery) -> List[VerificationEvidence]:
        """Verify financial claims using Yahoo Finance"""
        evidence = []
        
        try:
            # Extract ticker symbols from entities
            tickers = [entity.upper() for entity in query.entities if len(entity) <= 5 and entity.isalpha()]
            
            for ticker in tickers:
                try:
                    stock = yf.Ticker(ticker)
                    info = stock.info
                    history = stock.history(period="1d")
                    
                    if not history.empty:
                        current_price = history['Close'].iloc[-1]
                        
                        # Check if claim mentions price and validate
                        for value in query.numeric_values:
                            price_diff = abs(current_price - value) / current_price
                            supporting = price_diff < 0.05  # 5% tolerance
                            
                            evidence.append(VerificationEvidence(
                                source=source.name,
                                data={
                                    "ticker": ticker,
                                    "current_price": current_price,
                                    "claimed_price": value,
                                    "price_difference_pct": price_diff * 100,
                                    "volume": int(history['Volume'].iloc[-1]),
                                    "market_cap": info.get('marketCap', 'N/A')
                                },
                                confidence=source.reliability_score * (0.95 if supporting else 0.3),
                                timestamp=datetime.now(),
                                verification_method="yahoo_finance_api",
                                supporting=supporting
                            ))
                
                except Exception as e:
                    logger.error(f"Yahoo Finance verification failed for {ticker}: {str(e)}")
        
        except Exception as e:
            logger.error(f"Yahoo Finance verification error: {str(e)}")
        
        return evidence
    
    async def _verify_fred(self, source: VerificationSource, query: FactCheckQuery) -> List[VerificationEvidence]:
        """Verify economic data using Federal Reserve FRED API"""
        evidence = []
        
        try:
            # Map common economic terms to FRED series IDs
            fred_series_map = {
                'gdp': 'GDP',
                'unemployment': 'UNRATE',
                'inflation': 'CPIAUCSL',
                'interest rate': 'FEDFUNDS',
                'fed funds': 'FEDFUNDS'
            }
            
            for entity in query.entities:
                entity_lower = entity.lower()
                for term, series_id in fred_series_map.items():
                    if term in entity_lower:
                        # Simulate FRED API call (would be actual API call in production)
                        simulated_data = {
                            "series_id": series_id,
                            "latest_value": np.random.uniform(1, 10),
                            "date": "2025-11-01",
                            "units": "Percent" if "rate" in term else "Billions"
                        }
                        
                        # Validate against claimed values
                        for value in query.numeric_values:
                            supporting = abs(simulated_data["latest_value"] - value) < 1.0
                            
                            evidence.append(VerificationEvidence(
                                source=source.name,
                                data=simulated_data,
                                confidence=source.reliability_score * (0.9 if supporting else 0.2),
                                timestamp=datetime.now(),
                                verification_method="fred_api",
                                supporting=supporting
                            ))
        
        except Exception as e:
            logger.error(f"FRED verification error: {str(e)}")
        
        return evidence
    
    async def _verify_sec_edgar(self, source: VerificationSource, query: FactCheckQuery) -> List[VerificationEvidence]:
        """Verify corporate data using SEC EDGAR"""
        evidence = []
        
        try:
            # Extract potential company names or tickers
            companies = [entity for entity in query.entities if len(entity) > 1]
            
            for company in companies:
                # Simulate SEC filing data (would be actual EDGAR API in production)
                simulated_filing = {
                    "company_name": company,
                    "filing_type": "10-K",
                    "filing_date": "2025-10-30",
                    "revenue": np.random.uniform(1000000000, 50000000000),
                    "net_income": np.random.uniform(100000000, 5000000000)
                }
                
                # Check against numeric claims
                for value in query.numeric_values:
                    # Check if claimed value is close to filing data
                    revenue_match = abs(simulated_filing["revenue"] - value) / simulated_filing["revenue"] < 0.1
                    income_match = abs(simulated_filing["net_income"] - value) / simulated_filing["net_income"] < 0.1
                    
                    supporting = revenue_match or income_match
                    
                    evidence.append(VerificationEvidence(
                        source=source.name,
                        data=simulated_filing,
                        confidence=source.reliability_score * (0.85 if supporting else 0.25),
                        timestamp=datetime.now(),
                        verification_method="sec_edgar",
                        supporting=supporting
                    ))
        
        except Exception as e:
            logger.error(f"SEC EDGAR verification error: {str(e)}")
        
        return evidence
    
    def _calculate_overall_confidence(self, supporting: List[VerificationEvidence], 
                                    contradicting: List[VerificationEvidence]) -> float:
        """Calculate overall confidence score"""
        if not supporting and not contradicting:
            return 0.0
        
        # Weighted confidence based on source reliability and evidence strength
        total_supporting = sum(e.confidence for e in supporting)
        total_contradicting = sum(e.confidence for e in contradicting)
        
        if total_supporting + total_contradicting == 0:
            return 0.0
        
        # Base confidence on supporting evidence ratio
        base_confidence = total_supporting / (total_supporting + total_contradicting)
        
        # Adjust for number of sources
        source_bonus = min(len(supporting) * 0.1, 0.3)  # Up to 30% bonus
        contradiction_penalty = len(contradicting) * 0.15  # 15% penalty per contradiction
        
        final_confidence = base_confidence + source_bonus - contradiction_penalty
        return max(0.0, min(1.0, final_confidence))
    
    def _assess_risk(self, confidence: float, contradictions: int, supporting: int) -> str:
        """Assess risk level based on verification results"""
        if confidence >= 0.9 and contradictions == 0:
            return "LOW"
        elif confidence >= 0.75 and contradictions <= 1:
            return "MEDIUM"
        else:
            return "HIGH"
    
    def _assess_evidence_quality(self, evidence: List[VerificationEvidence]) -> str:
        """Assess overall evidence quality"""
        if not evidence:
            return "UNKNOWN"
        
        avg_confidence = sum(e.confidence for e in evidence) / len(evidence)
        
        if avg_confidence >= 0.85:
            return "STRONG"
        elif avg_confidence >= 0.65:
            return "MODERATE"
        else:
            return "WEAK"
    
    def _generate_warnings(self, query: FactCheckQuery, evidence: List[VerificationEvidence], 
                          confidence: float) -> List[str]:
        """Generate warnings based on verification results"""
        warnings = []
        
        if confidence < 0.5:
            warnings.append("Low confidence in verification - manual review recommended")
        
        contradicting = [e for e in evidence if not e.supporting]
        if len(contradicting) > 2:
            warnings.append(f"Multiple contradictory sources found ({len(contradicting)} sources)")
        
        if len(evidence) < query.sources_required:
            warnings.append(f"Insufficient sources consulted ({len(evidence)} of {query.sources_required} required)")
        
        if not query.numeric_values and query.claim_type in ["financial", "economic", "statistical"]:
            warnings.append("No numeric values found in quantitative claim - may affect verification accuracy")
        
        return warnings
    
    def _generate_recommendations(self, query: FactCheckQuery, evidence: List[VerificationEvidence],
                                verified: bool) -> List[str]:
        """Generate recommendations based on verification results"""
        recommendations = []
        
        if not verified:
            recommendations.append("Consider seeking additional sources before relying on this claim")
            recommendations.append("Verify with authoritative sources directly")
        
        if len(evidence) > 0:
            best_sources = sorted(evidence, key=lambda x: x.confidence, reverse=True)[:3]
            source_names = [e.source for e in best_sources]
            recommendations.append(f"Most reliable sources for this claim type: {', '.join(source_names)}")
        
        if query.claim_type == "financial":
            recommendations.append("For financial data, consider checking multiple exchanges and official filings")
        
        return recommendations
    
    def _check_cache(self, claim: str) -> Optional[ComprehensiveFactCheck]:
        """Check if verification is cached"""
        if not self.conn:
            return None
        
        try:
            claim_hash = hashlib.md5(claim.encode()).hexdigest()
            cursor = self.conn.execute(
                "SELECT verification_result FROM verification_cache WHERE claim_hash = ?",
                (claim_hash,)
            )
            result = cursor.fetchone()
            
            if result:
                return json.loads(result[0])  # Would need proper deserialization
            
        except Exception as e:
            logger.error(f"Cache check error: {str(e)}")
        
        return None
    
    def _is_cache_valid(self, cached_result: dict, max_age_hours: int = 24) -> bool:
        """Check if cached result is still valid"""
        try:
            cache_time = datetime.fromisoformat(cached_result.get('verification_timestamp', ''))
            return datetime.now() - cache_time < timedelta(hours=max_age_hours)
        except:
            return False
    
    def _cache_result(self, claim: str, result: ComprehensiveFactCheck):
        """Cache verification result"""
        if not self.conn:
            return
        
        try:
            claim_hash = hashlib.md5(claim.encode()).hexdigest()
            result_json = json.dumps(asdict(result), default=str)
            sources_used = ','.join([e.source for e in result.supporting_evidence + result.contradicting_evidence])
            
            self.conn.execute(
                "INSERT OR REPLACE INTO verification_cache VALUES (?, ?, ?, ?, ?, ?)",
                (claim_hash, claim, result_json, result.confidence_score, 
                 result.verification_timestamp.isoformat(), sources_used)
            )
            self.conn.commit()
            
        except Exception as e:
            logger.error(f"Cache storage error: {str(e)}")
    
    def _can_verify_claim_type(self, source: VerificationSource, claim_type: str) -> bool:
        """Check if source can verify this type of claim"""
        type_mapping = {
            "financial": ["finance", "markets", "stocks"],
            "economic": ["economic_indicators", "economic_data", "economics"],
            "corporate": ["corporate", "corporate_filings", "news"],
            "statistical": ["statistics", "demographics", "economic_indicators"]
        }
        
        applicable_types = type_mapping.get(claim_type, [])
        return any(data_type in source.data_types for data_type in applicable_types)
    
    def _is_rate_limited(self, source: VerificationSource) -> bool:
        """Check if we're rate limited for this source"""
        # Simple rate limiting - would be more sophisticated in production
        if source.last_used:
            time_since_last = datetime.now() - source.last_used
            min_interval = timedelta(seconds=60 / source.rate_limit)
            return time_since_last < min_interval
        return False

class EntityExtractor:
    """Extract entities (companies, indicators, etc.) from claims"""
    
    def extract_entities(self, claim: str) -> List[str]:
        """Extract relevant entities from claim text"""
        entities = []
        
        # Stock ticker patterns
        ticker_pattern = r'\b[A-Z]{1,5}\b'
        tickers = re.findall(ticker_pattern, claim)
        entities.extend(tickers)
        
        # Company name patterns (simplified)
        company_indicators = ['Inc', 'Corp', 'LLC', 'Ltd', 'Company', 'Technologies', 'Systems']
        words = claim.split()
        
        for i, word in enumerate(words):
            if word in company_indicators and i > 0:
                # Add the word before the indicator as potential company name
                entities.append(words[i-1])
        
        # Economic indicators
        economic_terms = ['GDP', 'unemployment', 'inflation', 'CPI', 'PPI', 'interest rate', 'fed funds']
        for term in economic_terms:
            if term.lower() in claim.lower():
                entities.append(term)
        
        return list(set(entities))  # Remove duplicates

class NumericValidator:
    """Extract and validate numeric values from claims"""
    
    def extract_numbers(self, claim: str) -> List[float]:
        """Extract numeric values from claim"""
        # Patterns for different number formats
        patterns = [
            r'\$\s*(\d+(?:,\d{3})*(?:\.\d{2})?)',  # Currency
            r'(\d+(?:,\d{3})*(?:\.\d+)?)\s*%',     # Percentages
            r'(\d+(?:,\d{3})*(?:\.\d+)?)',         # General numbers
        ]
        
        numbers = []
        for pattern in patterns:
            matches = re.findall(pattern, claim)
            for match in matches:
                try:
                    # Remove commas and convert to float
                    number = float(match.replace(',', ''))
                    numbers.append(number)
                except ValueError:
                    continue
        
        return numbers

class CrossReferencer:
    """Cross-reference data across sources for consistency"""
    
    def cross_reference(self, evidence_list: List[VerificationEvidence]) -> Dict:
        """Cross-reference evidence for consistency"""
        cross_ref_results = {
            'consistent_sources': [],
            'inconsistent_sources': [],
            'confidence_weighted_average': 0.0,
            'agreement_percentage': 0.0
        }
        
        if len(evidence_list) < 2:
            return cross_ref_results
        
        # Group by supporting vs contradicting
        supporting = [e for e in evidence_list if e.supporting]
        contradicting = [e for e in evidence_list if not e.supporting]
        
        # Calculate agreement
        total_sources = len(evidence_list)
        agreement_pct = len(supporting) / total_sources * 100
        cross_ref_results['agreement_percentage'] = agreement_pct
        
        # Identify consistent vs inconsistent sources
        if agreement_pct >= 70:
            cross_ref_results['consistent_sources'] = [e.source for e in supporting]
            cross_ref_results['inconsistent_sources'] = [e.source for e in contradicting]
        else:
            # If low agreement, flag all as inconsistent
            cross_ref_results['inconsistent_sources'] = [e.source for e in evidence_list]
        
        # Calculate weighted average confidence
        if evidence_list:
            total_weighted_confidence = sum(e.confidence for e in evidence_list)
            cross_ref_results['confidence_weighted_average'] = total_weighted_confidence / len(evidence_list)
        
        return cross_ref_results

# Example usage and testing
async def main():
    """Example usage of the advanced fact-checking system"""
    fact_checker = AdvancedFactChecker()
    
    # Test claims
    test_claims = [
        "Apple Inc (AAPL) stock price is currently $150.25",
        "US unemployment rate decreased to 3.7% in October 2025",
        "Tesla reported quarterly revenue of $23.4 billion in Q3 2025",
        "The Federal Reserve raised interest rates to 5.5% last month"
    ]
    
    print("üî¨ Advanced Fact-Checking Engine - Test Run")
    print("=" * 60)
    
    for i, claim in enumerate(test_claims, 1):
        print(f"\n{i}. Testing claim: {claim}")
        print("-" * 50)
        
        result = await fact_checker.comprehensive_fact_check(
            claim=claim,
            confidence_threshold=0.8,
            sources_required=2
        )
        
        print(f"Verified: {'‚úÖ YES' if result.overall_verified else '‚ùå NO'}")
        print(f"Confidence: {result.confidence_score:.2%}")
        print(f"Risk Level: {result.risk_assessment}")
        print(f"Evidence Quality: {result.evidence_quality}")
        print(f"Sources Consulted: {result.sources_consulted}")
        print(f"Processing Time: {result.processing_time:.2f}s")
        
        if result.warnings:
            print(f"‚ö†Ô∏è Warnings: {'; '.join(result.warnings)}")
        
        if result.recommendations:
            print(f"üí° Recommendations: {'; '.join(result.recommendations)}")

if __name__ == "__main__":
    asyncio.run(main())