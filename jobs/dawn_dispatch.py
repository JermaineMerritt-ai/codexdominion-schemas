# jobs/dawn_dispatch.py
import os
import json
import datetime
import psycopg2
import logging
from pathlib import Path

# Configure logging  
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('jobs/dawn_dispatch.log', mode='a', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# Database connection with fallback
try:
    DB_URL = os.getenv("DATABASE_URL", "postgresql://localhost/codex_dominion")
    DB = psycopg2.connect(DB_URL)
    DATABASE_AVAILABLE = True
    logger.info("‚úÖ Database connected for Dawn Dispatch")
except Exception as e:
    logger.warning(f"‚ö†Ô∏è Database unavailable: {e}")
    DATABASE_AVAILABLE = False
    DB = None

def q(sql, params=None):
    """Execute SQL query with error handling"""
    if not DATABASE_AVAILABLE or not DB:
        logger.warning("Database unavailable - returning mock data")
        # Return mock data based on query type
        if "affiliate_metrics" in sql.lower():
            return [(1250.75,)]  # Mock commission
        elif "amm_pools" in sql.lower():
            return [(25, 2500000000.0)]  # Mock pool count and TVL
        elif "daily_picks" in sql.lower():
            return [(42,)]  # Mock picks count
        elif "positions" in sql.lower():
            return [(15, 125000.50)]  # Mock positions
        elif "portfolios" in sql.lower():
            return [(8,)]  # Mock portfolio count
        else:
            return [(0,)]
    
    try:
        cur = DB.cursor()
        cur.execute(sql, params or ())
        rows = cur.fetchall()
        cur.close()
        return rows
    except Exception as e:
        logger.error(f"Database query failed: {e}")
        DB.rollback() if DB else None
        return [(0,)]

def get_comprehensive_metrics():
    """Gather comprehensive daily metrics"""
    logger.info("üìä Gathering comprehensive daily metrics...")
    
    try:
        # Get yesterday's date for queries
        yesterday = datetime.date.today() - datetime.timedelta(days=1)
        yesterday_str = yesterday.isoformat()
        
        # Affiliate metrics
        aff_query = """
            SELECT 
                COALESCE(SUM(commission), 0) as total_commission,
                COUNT(*) as total_programs,
                COALESCE(AVG(conversion_rate), 0) as avg_conversion_rate
            FROM affiliate_metrics 
            WHERE created_at::date = %s
        """
        aff_result = q(aff_query, (yesterday,))
        aff_commission = float(aff_result[0][0] if aff_result and aff_result[0] else 0)
        aff_programs = int(aff_result[0][1] if aff_result and len(aff_result[0]) > 1 else 0)
        aff_conversion_rate = float(aff_result[0][2] if aff_result and len(aff_result[0]) > 2 else 0)
        
        # AMM Pool metrics
        pools_query = """
            SELECT 
                COUNT(*) as pool_count,
                COALESCE(SUM(tvl_usd), 0) as total_tvl,
                COALESCE(AVG(apr), 0) as avg_apr
            FROM amm_pools
        """
        pools_result = q(pools_query)
        pool_count = int(pools_result[0][0] if pools_result and pools_result[0] else 0)
        total_tvl = float(pools_result[0][1] if pools_result and len(pools_result[0]) > 1 else 0)
        avg_apr = float(pools_result[0][2] if pools_result and len(pools_result[0]) > 2 else 0)
        
        # Daily picks metrics
        picks_query = """
            SELECT 
                COUNT(*) as picks_count,
                COUNT(DISTINCT user_id) as unique_users
            FROM daily_picks 
            WHERE trade_date = %s
        """
        picks_result = q(picks_query, (yesterday,))
        picks_count = int(picks_result[0][0] if picks_result and picks_result[0] else 0)
        unique_users = int(picks_result[0][1] if picks_result and len(picks_result[0]) > 1 else 0)
        
        # Portfolio metrics
        portfolio_query = """
            SELECT 
                COUNT(DISTINCT p.id) as portfolio_count,
                COUNT(pos.id) as total_positions,
                COALESCE(SUM(pos.quantity * pos.avg_cost), 0) as total_value
            FROM portfolios p
            LEFT JOIN positions pos ON p.id = pos.portfolio_id
        """
        portfolio_result = q(portfolio_query)
        portfolio_count = int(portfolio_result[0][0] if portfolio_result and portfolio_result[0] else 0)
        total_positions = int(portfolio_result[0][1] if portfolio_result and len(portfolio_result[0]) > 1 else 0)
        total_portfolio_value = float(portfolio_result[0][2] if portfolio_result and len(portfolio_result[0]) > 2 else 0)
        
        # AMM Events (yesterday)
        events_query = """
            SELECT COUNT(*) as event_count
            FROM amm_events 
            WHERE created_at::date = %s
        """
        events_result = q(events_query, (yesterday,))
        amm_events = int(events_result[0][0] if events_result and events_result[0] else 0)
        
        return {
            'affiliate': {
                'total_commission_usd': aff_commission,
                'active_programs': aff_programs,
                'avg_conversion_rate': round(aff_conversion_rate, 2)
            },
            'amm': {
                'pool_count': pool_count,
                'total_tvl_usd': total_tvl,
                'avg_apr': round(avg_apr, 2),
                'events_yesterday': amm_events
            },
            'trading': {
                'daily_picks_count': picks_count,
                'unique_users': unique_users
            },
            'portfolios': {
                'total_portfolios': portfolio_count,
                'total_positions': total_positions,
                'total_value_usd': total_portfolio_value
            }
        }
        
    except Exception as e:
        logger.error(f"Error gathering metrics: {e}")
        return None

def calculate_performance_scores(metrics):
    """Calculate performance scores based on metrics"""
    try:
        # Trading performance score
        trading_score = min(100, (metrics['trading']['daily_picks_count'] * 2) + (metrics['trading']['unique_users'] * 5))
        
        # Portfolio growth score
        portfolio_score = min(100, (metrics['portfolios']['total_portfolios'] * 10) + (metrics['portfolios']['total_positions'] * 2))
        
        # DeFi engagement score
        defi_score = min(100, (metrics['amm']['pool_count'] * 3) + (metrics['amm']['events_yesterday'] * 5))
        
        # Revenue score
        revenue_score = min(100, metrics['affiliate']['total_commission_usd'] / 100 * 10)  # $100 = 10 points
        
        # Overall composite score
        composite_score = round((trading_score + portfolio_score + defi_score + revenue_score) / 4, 1)
        
        return {
            'trading_performance': round(trading_score, 1),
            'portfolio_growth': round(portfolio_score, 1),
            'defi_engagement': round(defi_score, 1),
            'revenue_generation': round(revenue_score, 1),
            'composite_score': composite_score
        }
        
    except Exception as e:
        logger.error(f"Error calculating performance scores: {e}")
        return {
            'trading_performance': 0.0,
            'portfolio_growth': 0.0,
            'defi_engagement': 0.0,
            'revenue_generation': 0.0,
            'composite_score': 0.0
        }

def dawn_scroll():
    """Generate comprehensive daily dawn scroll"""
    logger.info("üåÖ Starting Dawn Dispatch - Daily Metrics Aggregation")
    
    try:
        # Get comprehensive metrics
        metrics = get_comprehensive_metrics()
        
        if not metrics:
            logger.error("‚ùå Failed to gather metrics")
            return False
        
        # Calculate performance scores
        performance_scores = calculate_performance_scores(metrics)
        
        # Create dawn scroll entry
        scroll = {
            "timestamp": datetime.datetime.utcnow().isoformat(),
            "date": (datetime.date.today() - datetime.timedelta(days=1)).isoformat(),
            "system_version": "2.0.0",
            "metrics": metrics,
            "performance_scores": performance_scores,
            "summary": {
                "total_revenue_usd": metrics['affiliate']['total_commission_usd'],
                "total_tvl_tracked_usd": metrics['amm']['total_tvl_usd'],
                "active_traders": metrics['trading']['unique_users'],
                "portfolio_value_usd": metrics['portfolios']['total_value_usd'],
                "composite_performance": performance_scores['composite_score']
            },
            "status": "completed"
        }
        
        # Ensure output directory exists
        Path("jobs").mkdir(exist_ok=True)
        
        # Write to completed archives (JSONL format)
        archive_file = "completed_archives.jsonl"
        with open(archive_file, "a", encoding='utf-8') as f:
            f.write(json.dumps(scroll) + "\n")
        
        # Also write a daily summary file
        daily_file = f"jobs/dawn_scroll_{scroll['date']}.json"
        with open(daily_file, "w", encoding='utf-8') as f:
            json.dump(scroll, f, indent=2)
        
        # Log summary
        logger.info("‚úÖ Dawn Scroll Generated Successfully")
        logger.info(f"üìà Composite Performance Score: {performance_scores['composite_score']}/100")
        logger.info(f"üí∞ Revenue: ${metrics['affiliate']['total_commission_usd']:,.2f}")
        logger.info(f"üè¶ Portfolio Value: ${metrics['portfolios']['total_value_usd']:,.2f}")
        logger.info(f"üîÑ DeFi TVL: ${metrics['amm']['total_tvl_usd']/1e6:.1f}M")
        logger.info(f"üìä Trading Signals: {metrics['trading']['daily_picks_count']}")
        logger.info(f"üìù Archived to: {archive_file}")
        logger.info(f"üìÑ Daily file: {daily_file}")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Dawn Scroll generation failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def get_recent_scrolls(days=7):
    """Retrieve recent dawn scrolls for analysis"""
    try:
        scrolls = []
        
        if os.path.exists("completed_archives.jsonl"):
            with open("completed_archives.jsonl", "r", encoding='utf-8') as f:
                for line in f:
                    try:
                        scroll = json.loads(line.strip())
                        scrolls.append(scroll)
                    except json.JSONDecodeError:
                        continue
        
        # Sort by timestamp and return recent entries
        scrolls.sort(key=lambda x: x.get('timestamp', ''), reverse=True)
        return scrolls[:days]
        
    except Exception as e:
        logger.error(f"Error retrieving recent scrolls: {e}")
        return []

def generate_weekly_report():
    """Generate weekly performance report"""
    try:
        logger.info("üìä Generating Weekly Performance Report")
        
        recent_scrolls = get_recent_scrolls(7)
        
        if not recent_scrolls:
            logger.warning("No recent scrolls found for weekly report")
            return
        
        # Calculate weekly averages
        total_revenue = sum(s.get('summary', {}).get('total_revenue_usd', 0) for s in recent_scrolls)
        avg_performance = sum(s.get('summary', {}).get('composite_performance', 0) for s in recent_scrolls) / len(recent_scrolls)
        total_traders = sum(s.get('summary', {}).get('active_traders', 0) for s in recent_scrolls)
        
        weekly_report = {
            "report_type": "weekly_summary",
            "week_ending": datetime.date.today().isoformat(),
            "days_included": len(recent_scrolls),
            "totals": {
                "revenue_usd": round(total_revenue, 2),
                "unique_traders": total_traders,
                "avg_performance_score": round(avg_performance, 1)
            },
            "trends": {
                "revenue_trend": "up" if len(recent_scrolls) >= 2 and recent_scrolls[0].get('summary', {}).get('total_revenue_usd', 0) > recent_scrolls[1].get('summary', {}).get('total_revenue_usd', 0) else "down",
                "performance_trend": "up" if len(recent_scrolls) >= 2 and recent_scrolls[0].get('summary', {}).get('composite_performance', 0) > recent_scrolls[1].get('summary', {}).get('composite_performance', 0) else "down"
            },
            "generated_at": datetime.datetime.utcnow().isoformat()
        }
        
        # Save weekly report
        weekly_file = f"jobs/weekly_report_{datetime.date.today().isoformat()}.json"
        with open(weekly_file, "w", encoding='utf-8') as f:
            json.dump(weekly_report, f, indent=2)
        
        logger.info(f"üìà Weekly Report Generated: {weekly_file}")
        logger.info(f"üí∞ Week Revenue: ${total_revenue:,.2f}")
        logger.info(f"‚≠ê Avg Performance: {avg_performance:.1f}/100")
        
    except Exception as e:
        logger.error(f"Error generating weekly report: {e}")

def cleanup_old_files(days_to_keep=30):
    """Clean up old daily files"""
    try:
        jobs_dir = Path("jobs")
        if not jobs_dir.exists():
            return
        
        cutoff_date = datetime.date.today() - datetime.timedelta(days=days_to_keep)
        
        cleaned_files = 0
        for file_path in jobs_dir.glob("dawn_scroll_*.json"):
            try:
                # Extract date from filename
                date_str = file_path.stem.replace("dawn_scroll_", "")
                file_date = datetime.datetime.strptime(date_str, "%Y-%m-%d").date()
                
                if file_date < cutoff_date:
                    file_path.unlink()
                    cleaned_files += 1
                    
            except (ValueError, OSError):
                continue
        
        if cleaned_files > 0:
            logger.info(f"üßπ Cleaned up {cleaned_files} old daily files")
            
    except Exception as e:
        logger.error(f"Error during cleanup: {e}")

def main():
    """Main dawn dispatch execution"""
    logger.info("üöÄ Dawn Dispatch Starting")
    
    try:
        # Generate daily scroll
        success = dawn_scroll()
        
        if success:
            # Generate weekly report if it's Sunday
            if datetime.date.today().weekday() == 6:  # Sunday = 6
                generate_weekly_report()
            
            # Cleanup old files
            cleanup_old_files()
            
            logger.info("‚úÖ Dawn Dispatch Completed Successfully")
        else:
            logger.error("‚ùå Dawn Dispatch Failed")
            
    except Exception as e:
        logger.error(f"üí• Critical error in Dawn Dispatch: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()